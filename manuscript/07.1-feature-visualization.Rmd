## Feature Visualization {#feature-visualization}

`r if(is.html){only.in.html}`


Convolutional neural networks learn abstract features from the raw image pixels.
Methods for feature visualization aim to visualize these features by generating images that maximally activate units of a trained convolutional neural network.

<!-- Background: Why feature visualization -->
One of the greatest strengths of deep neural networks is that they automatically learn high-level features in the hidden layers.
This partially reduces the need for feature engineering.
Let's say you want to build an image classifier with a support vector machine.
The raw pixel matrices are not the best input for training your SVM, so you create new features based on color, frequency domain, edge detectors and so on.
With convolutional neural networks, the image is fed into the network in its raw form (pixels), transformed multiple times, first through multiple convolutional layers and then through the fully connected layers and turned into a classification or prediction.
During training, the convolutional neural network learns new, increasingly complex features in its layers.

```{r fig.cap = "Architecture of Inception V1 neural network. Each enumerated unit (3a to 5b) represents a layer with differently sized convolutions and pooling. Figure from Olah, et al. 2019 (CC-BY 4.0) https://distill.pub/2017/activation-atlas/.", out.width = 800, include = FALSE}
knitr::include_graphics("images/inceptionv1.svg")
```
```{r fig.cap = "Features learned by a convolutional neural network (Inception V1) trained on the ImageNet data. The features range from simple features in the lower convolutional layers (left) to more abstract features in the higher convolutional layers (right). Figure from Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/appendix/.", out.width = 800}
knitr::include_graphics("images/cnn-features.png")
```

- The first convolutional layer(s) learn features such as edges and simple textures.
- Later convolutional layers learn features such as more complex textures and patterns.
- The last convolutional layers learn features such as objects or parts of objects.
- The fully connected layers learn to connect the activations from the high-level features to the individual classes to be predicted.

Cool.
But how do we actually get those hallucinatory images?

<!-- Feature Visualization explained-->
The approach of making the learned features explicit is called **Feature Visualization**.
Feature visualization for a unit of a neural network is done by finding the input that maximizes the activation of that unit.
"Unit" refers either to individual neurons, entire feature maps, entire (convolutional) layers or the final class probability in classification (or the corresponding pre-softmax neuron, which is recommended).
Individual neurons are atomic units of the network, so we would get the most information by creating feature visualizations for each neuron.
But there is a problem: 
Neural networks often contain millions of neurons and looking at each neuron's feature visualization would take too long.
The channels (sometimes called activation maps) as units are a good choice for feature visualization.
We can go one step further and visualize an entire convolutional layer.
Layers as a unit are used for Google's DeepDream, which repeatedly adds the visualized features of a layer to the original image, resulting in a dream-like version of the input.

```{r units, fig.cap="Feature visualization can be done for different units. A) Convolution neuron, B) Convolution channel, C) Convolution layer, D) Neuron, E) Hidden layer, F) Class probability neuron (or corresponding pre-softmax neuron)", out.width=800}
knitr::include_graphics("images/units.jpg")
```


```{r trippy, fig.cap="Optimized images for Inception V1 (channels mixed3a, mixed4c, mixed4d and mixed5a). Images are maximized for a random direction of the activations. Figure from Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/.", out.width=800, include = FALSE}
knitr::include_graphics("images/trippy.png")
```

### Feature Visualization through Optimization

In mathematical terms, feature visualization is an optimization problem.
We assume that the weights of the neural network are fixed, which means that the network is trained.
We are looking for a new image that maximizes the (mean) activation of a unit, here a single neuron:

$$img^*=\arg\max_{img}h_{n,x,y,z}(img)$$

The function $h$ is the activation of a neuron, *img* the input of the network (an image), x and y describe the spatial position of the neuron, n specifies the layer and z is the channel index.
For the mean activation of an entire channel z in layer n we maximize:

$$img^*=\arg\max_{img}\sum_{x,y}h_{n,x,y,z}(img)$$

In this formula, all neurons in channel z are equally weighted.
Alternatively, you can also maximize random directions, which means that the neurons would be multiplied by different parameters, including negative directions.
In this way, we study how the neurons interact within the channel.
Instead of maximizing the activation, you can also minimize the activation (which corresponds to maximizing the negative direction).
Interestingly, when you maximize the negative direction you get very different features for the same unit:

```{r pos-neg, fig.cap="Positive (left) and negative (right) activation of Inception V1 neuron 484 from layer mixed4d pre relu. While the neuron is maximally activated by wheels, something which seems to have eyes yields a negative activation. Code: https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/feature-visualization/negative_neurons.ipynb", out.width=800}
knitr::include_graphics("images/a484.png")
```

<!-- Why not use training data? -->
We can address this optimization problem in different ways.
First, why should we generate new images?
We could simply search through our training images and select those that maximize the activation.
This is a valid approach, but using training data has the problem that elements on the images can be correlated and we can't see what the neural network is really looking for.
If images that yield a high activation of a certain channel show a dog and a tennis ball, we don't know whether the neural network looks at the dog, the tennis ball or maybe at both.

<!-- Direct optimization --> 
The other approach is to generate new images, starting from random noise.
To obtain meaningful visualizations, there are usually constraints on the image, e.g. that only small changes are allowed.
To reduce noise in the feature visualization, you can apply jittering, rotation or scaling to the image before the optimization step.
Other regularization options include frequency penalization (e.g. reduce variance of neighboring pixels) or generating images with learned priors, e.g. with generative adversarial networks (GANs) [^synthesize] or denoising autoencoders [^plugandplay].

```{r activation-optim, fig.cap="Iterative optimization from random image to maximizing activation. Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/.", out.width=800}
knitr::include_graphics("images/activation-optim.png")
```


### Connection to Adversarial Examples

There is a connection between feature visualization and [adversarial examples](#adversarial):
Both techniques maximize the activation of a neural network unit.
For adversarial examples, we look for the maximum activation of the neuron for the adversarial (= incorrect) class.
One difference is the image we start with:
For adversarial examples, it is the image for which we want to generate the adversarial image.
For feature visualization it is, depending on the approach, random noise.



### Text and Tabular Data

The literature focuses on feature visualization for convolutional neural networks for image recognition.
Technically, there is nothing to stop you from finding the input that maximally activates a neuron of a fully connected neural network for tabular data or a recurrent neural network for text data.
You might not call it feature visualization any longer, since the "feature" would be a tabular data input or text.
For credit default prediction, the inputs might be the number of prior credits, number of mobile contracts, address and dozens of other features.
The learned feature of a neuron would then be a certain combination of the dozens of features.
For recurrent neural networks, it is a bit nicer to visualize what the network learned:
Karpathy et. al (2015)[^viz-rnn] showed that recurrent neural networks indeed have neurons that learn interpretable features.
They trained a character-level model, which predicts the next character in the sequence from the previous characters.
Once an opening brace "(" occurred, one of the neurons got highly activated, and got de-activated when the matching closing bracket ")" occurred.
Other neurons fired at the end of a line.
Some neurons fired in URLs.
The difference to the feature visualization for CNNs is that the examples were note found through optimization, but by studying neuron activations in the training data.

### Advantages

- Feature visualizations give **unique insight into the working of neural networks**, especially for image recognition.
Given the complexity and opacity of neural networks, feature visualization is an important step in analyzing and describing neural networks.
Through feature visualization, we learned that neural networks first learn simple edge and texture detectors and more abstract parts and objects in higher layers.
- Feature visualization is a great tool to **communicate in a non-technical way how neural networks work**.
-  Feature visualization can be **combined with feature attribution methods**, which explain which pixels were important for the classification.
The combination of both methods allows to explain an individual classification along with local visualization of the learned features that were involved in the classification.
See [The Building Blocks of Interpretability from distill.pub](https://distill.pub/2018/building-blocks/).
- Finally, feature visualizations make great desktop wallpapers and T-Shirts prints.

### Disadvantages

- **Many feature visualization images are not interpretable** at all, but contain some abstract features for which we have no words or mental concept.
  The display of feature visualizations along with training data can help.
 The images still might not reveal what the neural network reacted to and only show something like "maybe there has to be something yellow in the images".
- There are **too many units to look at**, even when "only" visualizing the channel activations.
  For Inception V1 there are already over 5000 channels from 9 convolutional layers.
If you also want to show the negative activations plus a few images from the training data that maximally or minimally activate the channel (let's say 4 positive, 4 negative images), then you must already display more than 50 000 images.
  And you have not even started to investigate random directions or created a diverse set of multiple images for the channel activations.
- **Illusion of Interpretability?**
  These images can convey the illusion that we understand what the neural network is doing.
  But do we really understand what is going on in the neural network?
  Even if we look at hundreds or thousands of feature visualizations, we cannot understand the neural network.
 The neural network remains to incomprehensible: The neurons interact in a complex way, positive and negative activations are unrelated, multiple neurons might learn very similar features and for many of the features we do not have equivalent human concept.
  We must not fall into the trap of believing we fully understand neural networks just because we believe we saw that neuron 349 in layer 7 is activated by daisies. 




### Software and Further Material

There is an open-source implementation of feature visualization called [Lucid](https://github.com/tensorflow/lucid).
You can conveniently try it in your browser by using the notebook links that are provided on the Lucid Github page.
No additional software is required.

If you want to dive a lot deeper into feature visualization, take a look at the distill.pub online journal, especially the feature visualization post by Olah et al. [^distill-fv], from which I used many of the images, and also about the building blocks of interpretability [^distill-blocks].


[^distill-fv]: Olah, et al., "Feature Visualization", Distill, 2017.

[^distill-blocks]: Olah, et al., "The Building Blocks of Interpretability", Distill, 2018.

[^plugandplay]: Nguyen, Anh, et al. "Plug & play generative networks: Conditional iterative generation of images in latent space." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.

[^synthesize]: Nguyen, Anh, et al. "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks." Advances in Neural Information Processing Systems. 2016.

[^viz-rnn]: Karpathy, Andrej, Justin Johnson, and Li Fei-Fei. "Visualizing and understanding recurrent networks." arXiv preprint arXiv:1506.02078 (2015).

[^imagenet]: Olga Russakovsky*, Jia Deng*, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei. (* = equal contribution) ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015


